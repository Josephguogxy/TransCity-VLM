# trafficPrediction (SmartCityLLM) ‚Äî Multimodal Forecasting with Stage A/B/C + GRPO

[![License](https://img.shields.io/badge/license-See%20LICENSE-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.10%2B-brightgreen.svg)](#environment)
[![PyTorch](https://img.shields.io/badge/pytorch-required-orange.svg)](#environment)

A research-oriented codebase for **multimodal forecasting** (e.g., traffic flow / electricity load) with a **multi-stage training pipeline**:

- **Stage-A**: multimodal warm-up / alignment (train encoders & adapters; optionally freeze decoder)
- **Stage-B**: supervised multi-task training (prediction + reasoning)
- **Stage-C**: further tuning (e.g., MixLoRA FFN-only; task-balanced sampling)
- **GRPO** (optional): RL-style fine-tuning for reasoning quality (Qwen3-aligned rules)

> ‚ö†Ô∏è Data and checkpoints are not shipped. You must prepare your own JSONL datasets and model weights.

---

## üî• News

- [2025.12.30] Initial open-source release.

---

## ‚ú® Demos / Figures

All figures are stored in `assets/`.

**Fig. 1 ‚Äî Data-to-Reasoning Traceability**
<p align="center">
  <img src="assets/Data-To-Reasoning.svg" width="820" alt="Fig.1 Data-to-Reasoning Traceability"/>
</p>

**Fig. 2 ‚Äî Expert-Level Interpretability**
<p align="center">
  <img src="assets/Expert-Interpretability.svg" width="820" alt="Fig.2 Expert-Level Interpretability"/>
</p>

**Fig. 3 ‚Äî MoMExp Nets Interpretability**
<p align="center">
  <img src="assets/mapExplain.svg" width="820" alt="Fig.3 MoMExp Nets Interpretability"/>
</p>

**Fig. 4 ‚Äî Agent-driven Evidence Workflow (Plan DAG + Traceable Records)**
<p align="center">
  <img src="assets/agent-framework-A1.svg" width="920" alt="Fig.4 Agent-driven Evidence Workflow"/>
</p>

**Fig. 5 ‚Äî Safety Control System**
<p align="center">
  <img src="assets/SafetyControlSystem.svg" width="820" alt="Fig.5 Safety Control System"/>
</p>

---

## Brief Introduction

This repository implements a multimodal LLM-based forecasting workflow. Each sample can include:

- **Text chunks** (e.g., POI / News / Accident / HopSensor / HopBA)
- **Optional images** (paths/URIs)
- A **prompt** formatted for chat-style decoders (example: Qwen3 chat template)

The core pipeline uses:

- Hugging Face Transformers (`Trainer`, `TrainingArguments`)
- DeepSpeed ZeRO for distributed training (Stage-A/B scripts)
- A unified JSONL data schema normalized by `dataprocessing.py`
- Optional RL fine-tuning via GRPO

---

## Getting Started

### Table of Contents

- [Code Structure](#code-structure)
- [Environment](#environment)
- [Data Format](#data-format)
- [Training](#training)
  - [Stage A](#stage-a)
  - [Stage B](#stage-b)
  - [Stage C](#stage-c)
  - [GRPO RL (optional)](#grpo-rl-optional)
- [Inference & Evaluation](#inference--evaluation)
- [Serving / Integration](#serving--integration)
- [Tokenizer & Vocab Policy (No New Tokens)](#tokenizer--vocab-policy-no-new-tokens)
- [License Notices](#license-notices)
- [Citation](#citation)
- [Acknowledgements](#acknowledgements)

---

## Code Structure

A simplified layout (some folders omitted):

```text
TransCity-VLM
/
‚îú‚îÄ LICENSE_NOTICES.md
‚îú‚îÄ dataprocessing.py
‚îú‚îÄ ds_config_zero3.json
‚îú‚îÄ LICENSE
‚îú‚îÄ LICENSE-APACHE-2.0.txt
‚îú‚îÄ requirements.txt
‚îú‚îÄ README.md
‚îú‚îÄ ds_config_zero3_hfopt.json
‚îú‚îÄ ds_config_zero2.json
‚îú‚îÄ model/
‚îÇ  ‚îú‚îÄ builder.py
‚îÇ  ‚îú‚îÄ decoder_mixlora.py
‚îÇ  ‚îú‚îÄ mixlora_masked.py
‚îÇ  ‚îú‚îÄ language_model/
‚îÇ  ‚îÇ  ‚îî‚îÄ smartcity_llm.py
‚îÇ  ‚îú‚îÄ multimodal_encoder/
‚îÇ  ‚îÇ  ‚îú‚îÄ chunk_text_encoder.py
‚îÇ  ‚îÇ  ‚îú‚îÄ imagebind_encoder.py
‚îÇ  ‚îÇ  ‚îî‚îÄ imagebind/
‚îÇ  ‚îÇ     ‚îú‚îÄ data.py
‚îÇ  ‚îÇ     ‚îú‚îÄ requirements.txt
‚îÇ  ‚îÇ     ‚îú‚îÄ LICENSE-CC-BY-NC-4.0.txt
‚îÇ  ‚îÇ     ‚îú‚îÄ LICENSE
‚îÇ  ‚îÇ     ‚îú‚îÄ bpe/
‚îÇ  ‚îÇ     ‚îÇ  ‚îî‚îÄ bpe_simple_vocab_16e6.txt.gz
‚îÇ  ‚îÇ     ‚îî‚îÄ models/
‚îÇ  ‚îÇ        ‚îú‚îÄ helpers.py
‚îÇ  ‚îÇ        ‚îú‚îÄ imagebind_model.py
‚îÇ  ‚îÇ        ‚îú‚îÄ multimodal_preprocessors.py
‚îÇ  ‚îÇ        ‚îî‚îÄ transformer.py
‚îÇ  ‚îú‚îÄ multimodal_projector/
‚îÇ  ‚îÇ  ‚îú‚îÄ group.py
‚îÇ  ‚îÇ  ‚îú‚îÄ projector.py
‚îÇ  ‚îÇ  ‚îî‚îÄ vpma.py
‚îÇ  ‚îî‚îÄ reinforcement_learning/
‚îÇ     ‚îú‚îÄ config.py
‚îÇ     ‚îú‚îÄ logprobs.py
‚îÇ     ‚îú‚îÄ loss.py
‚îÇ     ‚îú‚îÄ rewards.py
‚îÇ     ‚îî‚îÄ rollout.py
‚îú‚îÄ traffic_service/
‚îÇ  ‚îú‚îÄ cli.py
‚îÇ  ‚îú‚îÄ config.py
‚îÇ  ‚îú‚îÄ schemas.py
‚îÇ  ‚îú‚îÄ agents/
‚îÇ  ‚îÇ  ‚îú‚îÄ base.py
‚îÇ  ‚îÇ  ‚îú‚îÄ bootstrap/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ geocode_forward.py
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ nl_parse.py
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ normalize.py
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ time_window.py
‚îÇ  ‚îÇ  ‚îú‚îÄ demographics/
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ demographics.py
‚îÇ  ‚îÇ  ‚îú‚îÄ geo/
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ geocode_reverse.py
‚îÇ  ‚îÇ  ‚îú‚îÄ osm/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ poi.py
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ roads.py
‚îÇ  ‚îÇ  ‚îú‚îÄ record/
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ record_builder.py
‚îÇ  ‚îÇ  ‚îú‚îÄ satellite/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ fetch_gibs.py
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ store_mysql.py
‚îÇ  ‚îÇ  ‚îú‚îÄ traffic/
‚îÇ  ‚îÇ  ‚îÇ  ‚îú‚îÄ nearest_sensor.py
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ traffic_flow.py
‚îÇ  ‚îÇ  ‚îú‚îÄ weather/
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ weather.py
‚îÇ  ‚îÇ  ‚îî‚îÄ web/
‚îÇ  ‚îÇ     ‚îú‚îÄ content_fetcher.py
‚îÇ  ‚îÇ     ‚îú‚îÄ events.py
‚îÇ  ‚îÇ     ‚îú‚îÄ query_generator.py
‚îÇ  ‚îÇ     ‚îú‚îÄ scoring.py
‚îÇ  ‚îÇ     ‚îî‚îÄ web_search_agent.py
‚îÇ  ‚îú‚îÄ clients/
‚îÇ  ‚îÇ  ‚îú‚îÄ cds.py
‚îÇ  ‚îÇ  ‚îú‚îÄ census.py
‚îÇ  ‚îÇ  ‚îú‚îÄ gibs.py
‚îÇ  ‚îÇ  ‚îú‚îÄ http.py
‚îÇ  ‚îÇ  ‚îú‚îÄ nominatim.py
‚îÇ  ‚îÇ  ‚îú‚îÄ open_meteo.py
‚îÇ  ‚îÇ  ‚îú‚îÄ overpass.py
‚îÇ  ‚îÇ  ‚îú‚îÄ web_search.py
‚îÇ  ‚îÇ  ‚îî‚îÄ worldpop.py
‚îÇ  ‚îú‚îÄ core/
‚îÇ  ‚îÇ  ‚îú‚îÄ context.py
‚îÇ  ‚îÇ  ‚îú‚îÄ executor.py
‚îÇ  ‚îÇ  ‚îú‚îÄ logging.py
‚îÇ  ‚îÇ  ‚îú‚îÄ plan_builder.py
‚îÇ  ‚îÇ  ‚îî‚îÄ runner.py
‚îÇ  ‚îú‚îÄ db/
‚îÇ  ‚îÇ  ‚îú‚îÄ flow_repo.py
‚îÇ  ‚îÇ  ‚îú‚îÄ mysql_pool.py
‚îÇ  ‚îÇ  ‚îî‚îÄ satellite_images_repo.py
‚îÇ  ‚îú‚îÄ llm/
‚îÇ  ‚îÇ  ‚îú‚îÄ clients.py
‚îÇ  ‚îÇ  ‚îî‚îÄ prompts/
‚îÇ  ‚îÇ     ‚îî‚îÄ __init__.py
‚îÇ  ‚îî‚îÄ utils/
‚îÇ     ‚îú‚îÄ geo.py
‚îÇ     ‚îî‚îÄ time.py
‚îú‚îÄ trl/
‚îú‚îÄ utils/
‚îÇ  ‚îî‚îÄ mixed_pr_sampler.py
‚îú‚îÄ train/
‚îÇ  ‚îú‚îÄ train_mm_stageA.py
‚îÇ  ‚îú‚îÄ train_mm_stageB.py
‚îÇ  ‚îú‚îÄ train_mm_stageC.py
‚îÇ  ‚îú‚îÄ train_mm_stageD.py
‚îÇ  ‚îú‚îÄ run_grpo.sh
‚îÇ  ‚îú‚îÄ run_stageA_ds.sh
‚îÇ  ‚îú‚îÄ run_stageB_ds.sh
‚îÇ  ‚îî‚îÄ run_stageC_ds.sh
‚îî‚îÄ eval/
   ‚îú‚îÄ inference_mm_stageB.py
   ‚îú‚îÄ run_inference_stageB.sh
   ‚îú‚îÄ inference_mm_stageA.py
   ‚îú‚îÄ inference_mm_stageC.py
   ‚îú‚îÄ run_inference_stageA.sh
   ‚îî‚îÄ run_inference_stageC.sh
```

---

## Environment

### Option 1: venv

```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

### Option 2: conda (example)

```bash
conda create -n trafficPrediction python=3.10 -y
conda activate trafficPrediction
pip install -r requirements.txt
```

> Tip: For Stage-A/B DeepSpeed training, you need a working CUDA + NCCL environment.

---

## Training

> All `run_*.sh` scripts are templates: they avoid internal absolute paths and rely on env vars / editable defaults.

### Stage A

```bash
export DECODER_NAME_OR_PATH="your-decoder-model-or-local-path"
export ENCODER_NAME_OR_PATH="your-encoder-model-or-local-path"
export TRAIN_FILES="dataset_stageA/train_understand_dataset.jsonl"
export VAL_FILES="dataset_stageA/test_understand_dataset.jsonl"
export OUTPUT_DIR="outputs/stageA"

bash run_stageA_ds.sh
```

### Stage B

```bash
export DECODER_NAME_OR_PATH="your-decoder-model-or-local-path"
export ENCODER_NAME_OR_PATH="your-encoder-model-or-local-path"
export FULL_MODEL_LOAD_DIR="outputs/stageA"
export TRAIN_FILES="dataset_stageB/train_*.jsonl"
export VAL_FILES="dataset_stageB/val_*.jsonl"
export OUTPUT_DIR="outputs/stageB"

bash run_stageB_ds.sh
```

### Stage C

```bash
export DECODER_NAME_OR_PATH="your-decoder-model-or-local-path"
export ENCODER_NAME_OR_PATH="your-encoder-model-or-local-path"
export FULL_MODEL_LOAD_DIR="outputs/stageB"
export TRAIN_FILES="dataset_stageC/train_*.jsonl"
export VAL_FILES="dataset_stageC/val_*.jsonl"
export OUTPUT_DIR="outputs/stageC"

bash run_stageC_ds.sh
```

### GRPO RL

```bash
export SFT_DIR="outputs/stageC"
export TRAIN_FILES="dataset_rl/train_reason.jsonl"
export OUTPUT_DIR="outputs/grpo"

bash run_grpo.sh
```

---

## Inference & Evaluation

This repo typically evaluates by:

1) generating text outputs,
2) extracting numeric sequences,
3) computing MAE/MSE/RMSE/MAPE/wMAPE,
4) saving `labels_*.txt`, `preds_*.txt`, `metrics_*.json`, and per-rank `raw_*.jsonl`.

---

## License Notices

This repository may contain **multiple licenses**:

- See `LICENSE` for the repository-level license.
- See `THIRD_PARTY_NOTICES.md` for bundled/third-party components and their licenses.
- Some components (e.g., certain multimodal encoders) may be **non-commercial** ‚Äî check carefully before commercial usage.

---

## Citation

If you use this repository in your work, you can cite it as:

```bibtex
@misc{trafficPrediction,
  title        = {trafficPrediction: SmartCityLLM Multimodal Forecasting},
  author       = {torchtorch Authors},
  year         = {2025},
  howpublished = {https://github.com/<your-org-or-user>/trafficPrediction},
}
```

---

## Acknowledgements

This codebase builds on open-source libraries such as:

- PyTorch
- Hugging Face Transformers / Datasets
- DeepSpeed (for distributed training)
- (Optional) Accelerate / TRL for RL-related utilities

We thank the open-source community for these tools.